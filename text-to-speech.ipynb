{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ddebfb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paxton/miniconda3/envs/tts-breezy-voice/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tts import TTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cea6d20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_path = \"data/佑希-新聞播報-ai.wav\"\n",
    "prompt_text = \"以下是今天的新聞播報，美國蘋果公司在今天發表了新的 iPhone 17 系列，分別有 iPhone 17、iPhone 17 Plus、iPhone 17 Pro 和 iPhone 17 Pro Max，此次發表會還發表了全新升級的 AI 功能，讓 iPhone 17 系列有了比 siri 更加強大且實用的 AI 能力。\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d41a43c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 154 files: 100%|██████████| 154/154 [00:00<00:00, 11276.98it/s]\n",
      "/home/paxton/miniconda3/envs/tts-breezy-voice/lib/python3.10/site-packages/torch/_jit_internal.py:874: FutureWarning: ignore(True) has been deprecated. TorchScript will now drop the function call on compilation. Use torch.jit.unused now. {}\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model /home/paxton/.cache/huggingface/hub/models--MediaTek-Research--BreezyVoice-300M/snapshots/e33b502e0ac21c16b0ee0d00df66ac3fa737393d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paxton/miniconda3/envs/tts-breezy-voice/lib/python3.10/site-packages/diffusers/models/lora.py:393: FutureWarning: `LoRACompatibleLinear` is deprecated and will be removed in version 1.0.0. Use of `LoRACompatibleLinear` is deprecated. Please switch to PEFT backend by installing PEFT: `pip install peft`.\n",
      "  deprecate(\"LoRACompatibleLinear\", \"1.0.0\", deprecation_message)\n",
      "/home/paxton/miniconda3/envs/tts-breezy-voice/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "/home/paxton/code/llm/BreezyVoice/cosyvoice/dataset/processor.py:24: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  torchaudio.set_audio_backend('soundfile')\n",
      "/home/paxton/miniconda3/envs/tts-breezy-voice/lib/python3.10/site-packages/torchaudio/_internal/module_utils.py:71: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  return func(*args, **kwargs)\n",
      "/home/paxton/miniconda3/envs/tts-breezy-voice/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:121: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'AzureExecutionProvider, CPUExecutionProvider'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load leagacy transf breakmodel\n",
      "load leagacy transf breakmodel\n",
      "text.cc: festival_Text_init\n",
      "open voice lang map failed\n",
      "set used by tts frd\n",
      "Create TikTokenTokenizer with config_path=/home/paxton/.cache/huggingface/hub/models--MediaTek-Research--BreezyVoice-300M/snapshots/e33b502e0ac21c16b0ee0d00df66ac3fa737393d/CosyVoice-ttsfrd/resource/tokenizer/tiktoken.toml config_name=multilingual rank_path=/home/paxton/.cache/huggingface/hub/models--MediaTek-Research--BreezyVoice-300M/snapshots/e33b502e0ac21c16b0ee0d00df66ac3fa737393d/CosyVoice-ttsfrd/resource/tokenizer/multilingual.tiktoken\n",
      "load tokenizer failed\n",
      "break model index not valid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "failed to do test: failed to open file: /home/paxton/.cache/huggingface/hub/models--MediaTek-Research--BreezyVoice-300M/snapshots/e33b502e0ac21c16b0ee0d00df66ac3fa737393d/CosyVoice-ttsfrd/resource/tokenizer/tiktoken.toml\n",
      "/home/paxton/miniconda3/envs/tts-breezy-voice/lib/python3.10/site-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n",
      "/home/paxton/miniconda3/envs/tts-breezy-voice/lib/python3.10/site-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n"
     ]
    }
   ],
   "source": [
    "tts = TTS(prompt_path, prompt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dcc7209",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ['您先收聽的是公視每日新聞',\n",
    " '我是蔡惠玲',\n",
    " '帶您一同來關心 8 月 19 號的重點新聞',\n",
    " '烏尅蘭總統澤倫斯基 18 號再訪美國白宮和美國總統川普',\n",
    " '以及多位歐洲領袖談論俄烏戰爭議題',\n",
    " '雖然相比今年 2 月底',\n",
    " '川普和澤倫斯基公開爭執的火爆場面',\n",
    " '這次會談明顯氣氛融洽',\n",
    " '不過對於要終結俄烏戰火以及烏克蘭所期盼獲得安全保障還有多個關鍵問題',\n",
    " '幾乎都沒有具體的答案',\n",
    " '而澤倫斯基在會後則表示',\n",
    " '他相信俄羅斯總統普亭有意結束戰爭',\n",
    " '並且考慮推動美烏俄三方會談以尋求和平',\n",
    " '台灣科技業面臨美國對等關稅',\n",
    " '以及半導體關稅等考驗',\n",
    " '鴻海董事長暨電電工會理事長劉揚偉',\n",
    " '菅則表示',\n",
    " '大約有 25％ 的會員會受到影響',\n",
    " '研擬在美國和其他開發中國家建立科學園區',\n",
    " '而受到關稅衝擊',\n",
    " '勞動部最新的統計',\n",
    " '製造業實施五星架的架數已經達到了 146 架',\n",
    " '人數超過 3500 人',\n",
    " '而台灣隆澤科技近期也宣布',\n",
    " '實施為期三個月的每週五休假',\n",
    " '台北捷運車廂今天上午在通勤時間發生冒煙驚魂',\n",
    " '經過調查',\n",
    " '是一名旅客的手機冒煙',\n",
    " '北捷引導旅客下車',\n",
    " '並且使用滅火器撲滅',\n",
    " '後續確認車廂內焦味已經消散',\n",
    " '車廂也清理完畢',\n",
    " '並沒有影響到營運',\n",
    " '北捷則是呼籲民眾要留意隨身物品',\n",
    " '避免影響公共安全',\n",
    " '如果因為手機自然影響到行車運轉',\n",
    " '恐怕會觸犯公共危險罪',\n",
    " '中央普發現今',\n",
    " '臺南市議會不分藍綠市議員在臨時會就提案',\n",
    " '呼籲臺南市政府',\n",
    " '將超徵稅額',\n",
    " '還有超收的交通違規罰款',\n",
    " '用現金普發的方式還給市民',\n",
    " '國民黨團更建議直接發 1000 元',\n",
    " '對此市長黃偉哲則回應',\n",
    " '市府會進行研議',\n",
    " '9 月 26 號之前提出完整報告',\n",
    " '核三重啟公投在這星期六就要進行投開票',\n",
    " '反對重啟的學者和專家',\n",
    " '今天是召開記者會',\n",
    " '重申活動斷層經過核三廠區',\n",
    " '機組老化等 7 項反對理由',\n",
    " '另一邊的公投正方代表',\n",
    " '合秀董事長童子賢',\n",
    " '今天再度強調',\n",
    " '台灣需要價格合理的墊',\n",
    " '呼籲民眾要投下支持和三重起的同意票',\n",
    " '以上由公視新聞製作',\n",
    " '謝謝您的收聽']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ff86fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthesizing: 您先收聽的是公視每日新聞。\n",
      "max value is  tensor(1.0008)\n",
      "Synthesizing: 我是蔡惠玲。\n",
      "max value is  tensor(1.0008)\n",
      "Synthesizing: 帶您一同來關心八月十九號的重點新聞。\n",
      "max value is  tensor(1.0008)\n",
      "Synthesizing: 烏尅[:ㄎㄜ4]蘭總統澤倫斯基十八號再訪美國白宮和美國總統川普。\n",
      "max value is  tensor(1.0008)\n",
      "Synthesizing: 以及多位歐洲領袖談論俄烏戰爭議題。\n",
      "max value is  tensor(1.0008)\n",
      "Synthesizing: 雖然相比今年二月底。\n",
      "max value is  tensor(1.0008)\n",
      "Synthesizing: 川普和澤倫斯基公開爭執的火爆場面。\n",
      "max value is  tensor(1.0008)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, text \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data):\n\u001b[1;32m      7\u001b[0m     output_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m     \u001b[43mtts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/llm/BreezyVoice/tts.py:29\u001b[0m, in \u001b[0;36mTTS.generate\u001b[0;34m(self, generate_text, output_path)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# 生成語音\u001b[39;00m\n\u001b[1;32m     28\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 29\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference_zero_shot_no_normalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent_to_synthesize_bopomo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspeaker_prompt_text_transcription_bopomo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompt_speech_16k\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     31\u001b[0m torchaudio\u001b[38;5;241m.\u001b[39msave(output_path, output[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtts_speech\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;241m22050\u001b[39m)\n",
      "File \u001b[0;32m~/code/llm/BreezyVoice/single_inference.py:273\u001b[0m, in \u001b[0;36mCustomCosyVoice.inference_zero_shot_no_normalize\u001b[0;34m(self, tts_text, prompt_text, prompt_speech_16k)\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSynthesizing:\u001b[39m\u001b[38;5;124m\"\u001b[39m,i)\n\u001b[0;32m--> 273\u001b[0m model_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrontend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrontend_zero_shot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_speech_16k\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m model_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39minference(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_input)\n\u001b[1;32m    275\u001b[0m tts_speeches\u001b[38;5;241m.\u001b[39mappend(model_output[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtts_speech\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/code/llm/BreezyVoice/single_inference.py:117\u001b[0m, in \u001b[0;36mCustomCosyVoiceFrontEnd.frontend_zero_shot\u001b[0;34m(self, tts_text, prompt_text, prompt_speech_16k)\u001b[0m\n\u001b[1;32m    115\u001b[0m prompt_speech_22050 \u001b[38;5;241m=\u001b[39m torchaudio\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mResample(orig_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16000\u001b[39m, new_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m22050\u001b[39m)(prompt_speech_16k)\n\u001b[1;32m    116\u001b[0m speech_feat, speech_feat_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extract_speech_feat(prompt_speech_22050)\n\u001b[0;32m--> 117\u001b[0m speech_token, speech_token_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract_speech_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_speech_16k\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extract_spk_embedding(prompt_speech_16k)\n\u001b[1;32m    119\u001b[0m model_input \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m: tts_text_token, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_len\u001b[39m\u001b[38;5;124m'\u001b[39m: tts_text_token_len,\n\u001b[1;32m    120\u001b[0m                \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt_text\u001b[39m\u001b[38;5;124m'\u001b[39m: prompt_text_token, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt_text_len\u001b[39m\u001b[38;5;124m'\u001b[39m: prompt_text_token_len,\n\u001b[1;32m    121\u001b[0m                \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mllm_prompt_speech_token\u001b[39m\u001b[38;5;124m'\u001b[39m: speech_token, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mllm_prompt_speech_token_len\u001b[39m\u001b[38;5;124m'\u001b[39m: speech_token_len,\n\u001b[1;32m    122\u001b[0m                \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflow_prompt_speech_token\u001b[39m\u001b[38;5;124m'\u001b[39m: speech_token, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflow_prompt_speech_token_len\u001b[39m\u001b[38;5;124m'\u001b[39m: speech_token_len,\n\u001b[1;32m    123\u001b[0m                \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt_speech_feat\u001b[39m\u001b[38;5;124m'\u001b[39m: speech_feat, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt_speech_feat_len\u001b[39m\u001b[38;5;124m'\u001b[39m: speech_feat_len,\n\u001b[1;32m    124\u001b[0m                \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mllm_embedding\u001b[39m\u001b[38;5;124m'\u001b[39m: embedding, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflow_embedding\u001b[39m\u001b[38;5;124m'\u001b[39m: embedding}\n",
      "File \u001b[0;32m~/code/llm/BreezyVoice/cosyvoice/cli/frontend.py:80\u001b[0m, in \u001b[0;36mCosyVoiceFrontEnd._extract_speech_token\u001b[0;34m(self, speech)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_extract_speech_token\u001b[39m(\u001b[38;5;28mself\u001b[39m, speech):\n\u001b[1;32m     79\u001b[0m     feat \u001b[38;5;241m=\u001b[39m whisper\u001b[38;5;241m.\u001b[39mlog_mel_spectrogram(speech, n_mels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)\n\u001b[0;32m---> 80\u001b[0m     speech_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspeech_tokenizer_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspeech_tokenizer_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m                                                            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspeech_tokenizer_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mflatten()\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     82\u001b[0m     speech_token \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([speech_token], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     83\u001b[0m     speech_token_len \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([speech_token\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/miniconda3/envs/tts-breezy-voice/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:273\u001b[0m, in \u001b[0;36mSession.run\u001b[0;34m(self, output_names, input_feed, run_options)\u001b[0m\n\u001b[1;32m    271\u001b[0m     output_names \u001b[38;5;241m=\u001b[39m [output\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs_meta]\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 273\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_feed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m C\u001b[38;5;241m.\u001b[39mEPFail \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_fallback:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "output_dir = \"results\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for idx, text in enumerate(data):\n",
    "    output_file = os.path.join(output_dir, f\"{idx}.wav\")\n",
    "    tts.generate(text, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43a74370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthesizing: 您先收聽的是公視每日新聞。\n",
      "max value is  tensor(1.0008)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paxton/miniconda3/envs/tts-breezy-voice/lib/python3.10/site-packages/torchaudio/_backend/utils.py:337: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.save_with_torchcodec` under the hood. Some parameters like format, encoding, bits_per_sample, buffer_size, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's encoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.encoders.AudioEncoder\n",
      "  warnings.warn(\n",
      "/home/paxton/miniconda3/envs/tts-breezy-voice/lib/python3.10/site-packages/torchaudio/_backend/ffmpeg.py:247: UserWarning: torio.io._streaming_media_encoder.StreamingMediaEncoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  s = torchaudio.io.StreamWriter(uri, format=muxer, buffer_size=buffer_size)\n"
     ]
    }
   ],
   "source": [
    "output_path = \"results/test.wav\"\n",
    "generate_text = \"您先收聽的是公視每日新聞\"\n",
    "tts.generate(generate_text, output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tts-breezy-voice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
